{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install cdlib scikit-learn pandas plotly hvplot community python-louvain networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from cdlib import algorithms, evaluation\n",
    "import csv\n",
    "# import sys\n",
    "# import itertools\n",
    "import numpy as np\n",
    "# from itertools import count\n",
    "# import holoviews as hv\n",
    "# import hvplot.networkx as hvnx\n",
    "# import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "# import random\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import rand_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import kendalltau\n",
    "from scipy.spatial import distance\n",
    "# from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "from networkx.algorithms import approximation\n",
    "# from networkx.algorithms import community\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "from networkx.algorithms.centrality import closeness_centrality\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.cm as cm\n",
    "from matplotlib.cm import get_cmap\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from sklearn.metrics import pair_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEGG Network Formation\n",
    "\n",
    "We employ the value-based construction with a delta of 0.85."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.85\n",
    "\n",
    "Dataset = []\n",
    "\n",
    "# open and read a tab-separated values (TSV) file, storing the first 19 columns in dataset\n",
    "with open('../dataset/sourced/kegg_ventures_cleaned.txt') as tsv:\n",
    "    for line in csv.reader(tsv, dialect=\"excel-tab\"):\n",
    "        Dataset.append(line[:21])\n",
    "\n",
    "# remove the header (if any) to ensure dataset contains only data\n",
    "Dataset = Dataset[1:]\n",
    "\n",
    "# calculates the number of nodes (n) based on the length of the dataset\n",
    "n = len(Dataset)\n",
    "print(n)\n",
    "\n",
    "# creates an empty Networkx graph G\n",
    "G = nx.Graph()\n",
    "\n",
    "# add nodes to the graph G, each with specific attributes derived from the dataset\n",
    "\n",
    "G.add_nodes_from([\n",
    "    (\n",
    "        i,  # node identifier, a unique number for each node\n",
    "        {\n",
    "            \"label\": Dataset[i][0],  # 'label' attribute, usually a name from the dataset\n",
    "            \"group\": Dataset[i][1:4],  # 'group' attribute, a category, converted to integer\n",
    "            \"data\": Dataset[i][4:]  # 'data' attribute, containing all remaining data points for the node\n",
    "\n",
    "        }\n",
    "    )\n",
    "    for i in range(n)  # loop through each item in the dataset, where 'n' is the total number of items\n",
    "])\n",
    "\n",
    "# define edges between nodes based on specific conditions\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "\n",
    "        #print(G.nodes[i])\n",
    "        #print(G.nodes[j])\n",
    "\n",
    "        # convert the 'data' attribute of both nodes to float for calculations\n",
    "        x = [float(xi) for xi in G.nodes[i][\"data\"]]\n",
    "        y = [float(yi) for yi in G.nodes[j][\"data\"]]\n",
    "\n",
    "        # calculate distances and correlation coefficients between nodes\n",
    "        d = distance.euclidean(x,y)\n",
    "        m = distance.minkowski(x,y)\n",
    "        pearson = np.corrcoef(x,y)[0][1]\n",
    "        spearman, ps = spearmanr(x,y)\n",
    "        kendall, pk = kendalltau(x,y)\n",
    "\n",
    "        # add an edge if the Pearson correlation coefficient is above the threshold (delta)\n",
    "        if pearson>=delta:\n",
    "            G.add_edge(i,j, euc = d, mink= m, weight = pearson, spearman = spearman, kendall = kendall)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "Degree = dict(G.degree)\n",
    "nx.set_node_attributes(G, Degree, 'degree')\n",
    "print(\"Number of Nodes\", len(G.nodes()))\n",
    "print(\"Number of Edges\", G.number_of_edges())\n",
    "print(\"Transitivity\", nx.transitivity(G))\n",
    "\n",
    "# calculate and print the average clustering coefficient\n",
    "print(approximation.average_clustering(G, trials=1000, seed=10))\n",
    "\n",
    "# identify connected components in the graph G and print their sizes and labels\n",
    "Connected_Components = [G.subgraph(c).copy() for c in nx.connected_components(G) if len(c)>1]\n",
    "\n",
    "# used to keep track of the size of the largest connected component found so far\n",
    "max = -1\n",
    "\n",
    "# store the actual connected component that is the largest\n",
    "BigC = []\n",
    "\n",
    "# if the size (len(c)) of the current component c is greater than the current max\n",
    "# update max with the new larger size\n",
    "# and update BigC to be this current component.\n",
    "\n",
    "for c in Connected_Components:\n",
    "    if len(c) > max:\n",
    "        max = len(c)\n",
    "        BigC = c\n",
    "\n",
    "# printing the size of the largest connected component\n",
    "print(\"Max Component with\", len(BigC))\n",
    "\n",
    "nx.set_node_attributes(G, -1, 'gn_grp')\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# total number of nodes in the original graph before removing singletons\n",
    "original_node_count = G.number_of_nodes()\n",
    "\n",
    "# create a subgraph that contains only nodes in connected components\n",
    "subG = nx.compose_all(Connected_Components)\n",
    "\n",
    "# total number of nodes in the subgraph after removing singletons\n",
    "subgraph_node_count = subG.number_of_nodes()\n",
    "\n",
    "# calculate and print the number of singletons removed\n",
    "singletons_removed = original_node_count - subgraph_node_count\n",
    "print(f\"Number of singletons removed: {singletons_removed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KEGG Network Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a color map, one color for each component\n",
    "components = list(nx.connected_components(subG))\n",
    "color_map = {}\n",
    "for i, component in enumerate(components):\n",
    "    for node in component:\n",
    "        color_map[node] = i\n",
    "colors = [color_map[node] for node in subG.nodes()]\n",
    "\n",
    "pos = nx.spring_layout(subG)\n",
    "\n",
    "# draw the graph\n",
    "plt.figure(figsize=(12, 10))\n",
    "nodes = nx.draw_networkx_nodes(subG, pos, alpha=0.8, node_color=colors, cmap=plt.get_cmap('viridis'))\n",
    "nx.draw_networkx_edges(subG, pos, alpha=0.5, edge_color='gray')\n",
    "\n",
    "# create a legend\n",
    "# generate a color for each component using the same colormap and normalization\n",
    "cmap = plt.get_cmap('viridis')\n",
    "norm = plt.Normalize(0, len(components))\n",
    "legend_colors = [cmap(norm(i)) for i in range(len(components))]  # Generate legend colors\n",
    "\n",
    "# Create a legend with a color box for each component\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label=f\"Community {i+1}\",\n",
    "                          markerfacecolor=legend_colors[i], markersize=10) for i in range(len(components))]\n",
    "\n",
    "plt.title('KEGG Network Formation')\n",
    "plt.axis('off')\n",
    "plt.legend(handles=legend_elements, title='Communities', bbox_to_anchor=(1, 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community Detection Auxiliary Functions\n",
    "\n",
    "These are the functions used during the Community Detection of our constructed network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_community(G, communities, title, output_dir, file_name, save_file=False):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    pos = nx.spring_layout(G)  # Recalculate layout to ensure all nodes are included\n",
    "\n",
    "    cmap = plt.get_cmap('viridis', len(communities))\n",
    "\n",
    "    for i, community in enumerate(communities):\n",
    "        if community:  # Check if community is not empty\n",
    "            # Filter out nodes not present in the graph\n",
    "            valid_nodes = [node for node in community if node in G]\n",
    "            nx.draw_networkx_nodes(G, pos, nodelist=valid_nodes, node_color=[cmap(i)], alpha=0.8)\n",
    "\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5, edge_color='gray')\n",
    "    legend_handles = [mpatches.Patch(color=cmap(i), label=f'Community {i + 1}') for i in range(len(communities))]\n",
    "    plt.legend(handles=legend_handles, loc='upper left', title=\"Communities\")\n",
    "    plt.title(f\"{title} Algorithm Communities\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if save_file:\n",
    "        plt.savefig(f\"{output_dir}/{file_name}/{title}.png\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def modified_rand_score(labels_true, labels_pred, nodes_true_list, nodes_pred_list):\n",
    "    \"\"\"\n",
    "    Compute the Modified Rand Index (MRI), an extension of the Rand Index\n",
    "    that accounts for missing elements between partitions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels_true : list\n",
    "        True labels corresponding to the nodes.\n",
    "    labels_pred : list\n",
    "        Predicted labels corresponding to the nodes.\n",
    "    nodes_true_list : list\n",
    "        An ordered list of nodes corresponding to labels_true.\n",
    "    nodes_pred_list : list\n",
    "        An ordered list of nodes corresponding to labels_pred.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mri : float\n",
    "        The Modified Rand Index score.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(labels_true)\n",
    "    total_pairs = n * (n - 1) // 2  # All possible unique node pairs\n",
    "    n_00, n_11, n_xx = 0, 0, 0  # Initialize counts\n",
    "\n",
    "    # Convert lists to sets for fast lookup\n",
    "    nodes_true_set = set(nodes_true_list)\n",
    "    nodes_pred_set = set(nodes_pred_list)\n",
    "\n",
    "    # Identify common nodes between the two partitions\n",
    "    common_nodes = nodes_true_set.intersection(nodes_pred_set)\n",
    "\n",
    "    # Iterate over all node pairs\n",
    "    for i, j in itertools.combinations(range(n), 2):  # Iterate over all unique pairs\n",
    "        node_i, node_j = nodes_true_list[i], nodes_true_list[j]\n",
    "\n",
    "        # If either node is missing from the common set, count this pair as n_xx.\n",
    "        if node_i not in common_nodes or node_j not in common_nodes:\n",
    "            n_xx += 1\n",
    "        else:\n",
    "            same_true = (labels_true[i] == labels_true[j])\n",
    "            same_pred = (labels_pred[i] == labels_pred[j])\n",
    "            if same_true and same_pred:\n",
    "                n_11 += 1  # Agreeing pair (same in both)\n",
    "            elif not same_true and not same_pred:\n",
    "                n_00 += 1  # Agreeing pair (different in both)\n",
    "\n",
    "    # Compute MRI Score\n",
    "    if total_pairs == 0:\n",
    "        return 1.0  # Perfect match in trivial cases\n",
    "\n",
    "    MRI = (n_00 + n_11 + n_xx) / total_pairs\n",
    "    return MRI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Community Detection\n",
    "\n",
    "Using our formed network, we employ the Paris and LFM algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Directory and Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"Paris - Graphs\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "Paris_results = pd.DataFrame(columns=[\n",
    "    \"Number of Communities\",\n",
    "    \"Nodes in Largest Community\",\n",
    "    \"Community Sizes\",\n",
    "    \"Singletons\",\n",
    "    \"Modularity\",\n",
    "    \"Closeness Centrality\",\n",
    "    \"ARI (Level 1)\",\n",
    "    \"ARI (Level 2)\",\n",
    "    \"RI (Level 1)\",\n",
    "    \"RI (Level 2)\",\n",
    "    \"MRI (Level 1)\",\n",
    "    \"MRI (Level 2)\",\n",
    "    \"Communities\",\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Paris Algorithm and Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming subG is your graph object and it's already defined\n",
    "# TODO: Figure out how to remove cycles from the subG graph\n",
    "paris_clusters = algorithms.paris(subG)\n",
    "\n",
    "# true labels for ARI calculation\n",
    "true_labels1 = [subG.nodes[n][\"group\"][0] for n in subG.nodes()]\n",
    "true_labels2 = [subG.nodes[n][\"group\"][1] for n in subG.nodes()]\n",
    "\n",
    "# prepare predicted_labels for ARI calculation\n",
    "predicted_labels_dict = {node: -1 for node in subG.nodes()}\n",
    "for i, community in enumerate(paris_clusters.communities):\n",
    "    for node in community:\n",
    "        predicted_labels_dict[node] = i\n",
    "predicted_labels_list = [predicted_labels_dict[node] for node in subG.nodes()]\n",
    "\n",
    "# ARI calculation\n",
    "ari_classifications_score1 = adjusted_rand_score(true_labels1, predicted_labels_list)\n",
    "ari_classifications_score2 = adjusted_rand_score(true_labels2, predicted_labels_list)\n",
    "\n",
    "# RI calculation\n",
    "ri_classifications_score1 = rand_score(true_labels1, predicted_labels_list)\n",
    "ri_classifications_score2 = rand_score(true_labels2, predicted_labels_list)\n",
    "\n",
    "# MRI calculation\n",
    "nodes_true_list = list(subG.nodes())  # List of nodes in the true partition\n",
    "nodes_pred_list = list(node for community in paris_clusters.communities for node in community)  # List of nodes in the predicted partition\n",
    "mri_classifications_score1 = modified_rand_score(true_labels1, predicted_labels_list, nodes_true_list, nodes_pred_list)\n",
    "mri_classifications_score2 = modified_rand_score(true_labels2, predicted_labels_list, nodes_true_list, nodes_pred_list)\n",
    "\n",
    "num_communities = len(paris_clusters.communities)\n",
    "community_list = [list(community) for community in paris_clusters.communities]\n",
    "community_sizes = [len(community) for community in paris_clusters.communities]\n",
    "\n",
    "# find the largest community size\n",
    "largest_community_size = 0\n",
    "for community in paris_clusters.communities:\n",
    "    if len(community) > largest_community_size:\n",
    "        largest_community_size = len(community)\n",
    "\n",
    "num_singletons = community_sizes.count(1)\n",
    "\n",
    "# Modularity score for Paris clusters\n",
    "modularity_score = evaluation.newman_girvan_modularity(subG, paris_clusters).score\n",
    "\n",
    "# Create subgraph using Paris clusters\n",
    "subgraph = subG.subgraph([node for community in paris_clusters.communities for node in community])\n",
    "\n",
    "# Calculate closeness centrality for all nodes in the subgraph\n",
    "centrality_score = nx.closeness_centrality(subgraph)\n",
    "\n",
    "# Calculate the overall closeness centrality score as the average of all node scores\n",
    "overall_centrality_score = sum(centrality_score.values()) / len(centrality_score)\n",
    "\n",
    "Paris_results.loc[0] = [\n",
    "    num_communities, \n",
    "    largest_community_size, \n",
    "    str(community_sizes), \n",
    "    num_singletons, \n",
    "    modularity_score, \n",
    "    overall_centrality_score, \n",
    "    ari_classifications_score1, \n",
    "    ari_classifications_score2, \n",
    "    ri_classifications_score1, \n",
    "    ri_classifications_score2,\n",
    "    mri_classifications_score1,\n",
    "    mri_classifications_score2,\n",
    "    community_list]\n",
    "\n",
    "# visualization function call\n",
    "visualize_community(subG, paris_clusters.communities, 'Paris', output_dir, 'Paris_Communities', save_file=False)\n",
    "\n",
    "plt.close()\n",
    "\n",
    "Paris_results.to_csv(f\"{output_dir}/Paris_Results_Kegg.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paris Algorithm Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Paris_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Community Detection\n",
    "Using our formed network, we employ the Infomap and TILES algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Directory and Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"Infomap - Graphs\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "Infomap_results = pd.DataFrame(columns=[\n",
    "    \"Number of Communities\",\n",
    "    \"Nodes in Largest Community\",\n",
    "    \"Community Sizes\",\n",
    "    \"Singletons\",\n",
    "    \"Modularity\",\n",
    "    \"Closeness Centrality\",\n",
    "    \"ARI (Level 1)\",\n",
    "    \"ARI (Level 2)\",\n",
    "    \"RI (Level 1)\",\n",
    "    \"RI (Level 2)\",\n",
    "    \"MRI (Level 1)\",\n",
    "    \"MRI (Level 2)\",\n",
    "    \"Communities\",\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Infomap Algorithm and Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming subG is your graph object and it's already defined\n",
    "infomap_clusters = algorithms.infomap(subG)\n",
    "\n",
    "# true labels for ARI calculation\n",
    "true_labels1 = [subG.nodes[n][\"group\"][0] for n in subG.nodes()]\n",
    "true_labels2 = [subG.nodes[n][\"group\"][1] for n in subG.nodes()]\n",
    "\n",
    "# prepare predicted_labels for ARI calculation\n",
    "predicted_labels_dict = {node: -1 for node in subG.nodes()}\n",
    "for i, community in enumerate(infomap_clusters.communities):\n",
    "    for node in community:\n",
    "        predicted_labels_dict[node] = i\n",
    "predicted_labels_list = [predicted_labels_dict[node] for node in subG.nodes()]\n",
    "\n",
    "# ARI calculation\n",
    "ari_classifications_score1 = adjusted_rand_score(true_labels1, predicted_labels_list)\n",
    "ari_classifications_score2 = adjusted_rand_score(true_labels2, predicted_labels_list)\n",
    "\n",
    "# RI calculation\n",
    "ri_classifications_score1 = rand_score(true_labels1, predicted_labels_list)\n",
    "ri_classifications_score2 = rand_score(true_labels2, predicted_labels_list)\n",
    "\n",
    "# MRI calculation\n",
    "nodes_true_list = list(subG.nodes())  # List of nodes in the true partition\n",
    "nodes_pred_list = list(node for community in infomap_clusters.communities for node in community)  # List of nodes in the predicted partition\n",
    "mri_classifications_score1 = modified_rand_score(true_labels1, predicted_labels_list, nodes_true_list, nodes_pred_list)\n",
    "mri_classifications_score2 = modified_rand_score(true_labels2, predicted_labels_list, nodes_true_list, nodes_pred_list)\n",
    "\n",
    "num_communities = len(infomap_clusters.communities)\n",
    "community_list = [list(community) for community in infomap_clusters.communities]\n",
    "community_sizes = [len(community) for community in infomap_clusters.communities]\n",
    "\n",
    "# find the largest community size\n",
    "largest_community_size = 0\n",
    "for community in infomap_clusters.communities:\n",
    "    if len(community) > largest_community_size:\n",
    "        largest_community_size = len(community)\n",
    "\n",
    "num_singletons = community_sizes.count(1)\n",
    "\n",
    "# Modularity score for Infomap clusters\n",
    "modularity_score = evaluation.newman_girvan_modularity(subG, infomap_clusters).score\n",
    "\n",
    "# Create subgraph using Infomap clusters\n",
    "subgraph = subG.subgraph([node for community in infomap_clusters.communities for node in community])\n",
    "\n",
    "# Calculate closeness centrality for all nodes in the subgraph\n",
    "centrality_score = nx.closeness_centrality(subgraph)\n",
    "\n",
    "# Calculate the overall closeness centrality score as the average of all node scores\n",
    "overall_centrality_score = sum(centrality_score.values()) / len(centrality_score)\n",
    "\n",
    "Infomap_results.loc[0] = [\n",
    "    num_communities, \n",
    "    largest_community_size, \n",
    "    str(community_sizes), \n",
    "    num_singletons, \n",
    "    modularity_score, \n",
    "    overall_centrality_score, \n",
    "    ari_classifications_score1, \n",
    "    ari_classifications_score2, \n",
    "    ri_classifications_score1, \n",
    "    ri_classifications_score2,\n",
    "    mri_classifications_score1,\n",
    "    mri_classifications_score2,\n",
    "    community_list\n",
    "    ]\n",
    "\n",
    "# visualization function call\n",
    "visualize_community(subG, infomap_clusters.communities, 'Infomap', output_dir, 'Infomap_Communities', save_file=False)\n",
    "\n",
    "plt.close()\n",
    "\n",
    "Infomap_results.to_csv(f\"{output_dir}/Infomap_Results_Kegg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Infomap_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
