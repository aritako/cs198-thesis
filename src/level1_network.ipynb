{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cdlib in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (5.22.0)\n",
      "Requirement already satisfied: hvplot in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: community in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.0b1)\n",
      "Requirement already satisfied: python-louvain in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.16)\n",
      "Requirement already satisfied: dynetx in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cdlib) (1.26.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cdlib) (4.67.1)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cdlib) (3.4.2)\n",
      "Requirement already satisfied: demon in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cdlib) (2.0.6)\n",
      "Requirement already satisfied: scipy>=1.10 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cdlib) (1.13.0)\n",
      "Requirement already satisfied: pulp in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cdlib) (2.9.0)\n",
      "Requirement already satisfied: eva-lcd in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cdlib) (0.1.1)\n",
      "Requirement already satisfied: bimlpa in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cdlib) (0.1.2)\n",
      "Requirement already satisfied: python-igraph>=0.10 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cdlib) (0.11.8)\n",
      "Requirement already satisfied: angelcommunity in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cdlib) (2.0.0)\n",
      "Requirement already satisfied: pooch in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cdlib) (1.8.2)\n",
      "Requirement already satisfied: thresholdclustering in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cdlib) (1.1)\n",
      "Requirement already satisfied: python-Levenshtein in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cdlib) (0.26.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from plotly) (8.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\calvs\\appdata\\roaming\\python\\python312\\site-packages (from plotly) (24.0)\n",
      "Requirement already satisfied: bokeh>=3.1 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from hvplot) (3.6.2)\n",
      "Requirement already satisfied: colorcet>=2 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from hvplot) (3.1.0)\n",
      "Requirement already satisfied: holoviews>=1.19.0 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from hvplot) (1.20.0)\n",
      "Requirement already satisfied: panel>=1.0 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from hvplot) (1.6.0)\n",
      "Requirement already satisfied: param<3.0,>=1.12.0 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from hvplot) (2.2.0)\n",
      "Requirement already satisfied: Flask in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from community) (3.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from community) (2.31.0)\n",
      "Requirement already satisfied: future in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dynetx) (1.0.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\calvs\\appdata\\roaming\\python\\python312\\site-packages (from dynetx) (5.1.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (3.8.4)\n",
      "Requirement already satisfied: Jinja2>=2.9 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bokeh>=3.1->hvplot) (3.1.3)\n",
      "Requirement already satisfied: contourpy>=1.2 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bokeh>=3.1->hvplot) (1.2.1)\n",
      "Requirement already satisfied: pillow>=7.1.0 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bokeh>=3.1->hvplot) (10.3.0)\n",
      "Requirement already satisfied: PyYAML>=3.10 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bokeh>=3.1->hvplot) (6.0.1)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\calvs\\appdata\\roaming\\python\\python312\\site-packages (from bokeh>=3.1->hvplot) (6.4)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bokeh>=3.1->hvplot) (2025.1.0)\n",
      "Requirement already satisfied: pyviz-comms>=2.1 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from holoviews>=1.19.0->hvplot) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
      "Requirement already satisfied: bleach in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from panel>=1.0->hvplot) (6.1.0)\n",
      "Requirement already satisfied: linkify-it-py in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from panel>=1.0->hvplot) (2.0.3)\n",
      "Requirement already satisfied: markdown in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from panel>=1.0->hvplot) (3.7)\n",
      "Requirement already satisfied: markdown-it-py in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from panel>=1.0->hvplot) (3.0.0)\n",
      "Requirement already satisfied: mdit-py-plugins in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from panel>=1.0->hvplot) (0.4.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from panel>=1.0->hvplot) (4.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: igraph==0.11.8 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-igraph>=0.10->cdlib) (0.11.8)\n",
      "Requirement already satisfied: texttable>=1.6.2 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from igraph==0.11.8->python-igraph>=0.10->cdlib) (1.7.0)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask->community) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask->community) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask->community) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.9 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask->community) (1.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\calvs\\appdata\\roaming\\python\\python312\\site-packages (from pooch->cdlib) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->community) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->community) (2024.2.2)\n",
      "Requirement already satisfied: Levenshtein==0.26.1 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-Levenshtein->cdlib) (0.26.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Levenshtein==0.26.1->python-Levenshtein->cdlib) (3.12.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\calvs\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->cdlib) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Jinja2>=2.9->bokeh>=3.1->hvplot) (2.1.5)\n",
      "Requirement already satisfied: webencodings in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bleach->panel>=1.0->hvplot) (0.5.1)\n",
      "Requirement already satisfied: uc-micro-py in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from linkify-it-py->panel>=1.0->hvplot) (1.0.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\calvs\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py->panel>=1.0->hvplot) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install cdlib scikit-learn pandas plotly hvplot community python-louvain dynetx seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'leidenalg', 'infomap', 'graph_tool', 'bayanpy', 'wurlitzer'}\n",
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'ASLPAw', 'pyclustering'}\n",
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'wurlitzer', 'leidenalg', 'infomap'}\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import dynetx as dn\n",
    "\n",
    "from cdlib import algorithms, evaluation\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import rand_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import kendalltau\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from networkx.algorithms import approximation\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "from networkx.algorithms.centrality import closeness_centrality\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from sklearn.metrics import pair_confusion_matrix\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "import colorsys\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 1 Network Formation\n",
    "\n",
    "We experiment with varying levels of delta, from (0.15, 0.5, 0.85)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the level 1 dataset\n",
    "file_path = \"../dataset/level_1/level_1.txt\"\n",
    "df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "\n",
    "level_2_df = pd.read_csv(\"../dataset/level_2/level_2.txt\", sep=\"\\t\")\n",
    "\n",
    "# Create a mapping of Gene to Level_2\n",
    "gene_to_level_2 = dict(zip(level_2_df[\"Gene\"], level_2_df[\"Level_2\"]))\n",
    "\n",
    "# Extract relevant columns\n",
    "genes = df[\"Gene\"]\n",
    "level_1_classes = df[\"Level_1\"]\n",
    "expression_data = df.iloc[:, 2:].values  # Extract time-series expression data\n",
    "\n",
    "\n",
    "# Compute Pearson correlation matrix\n",
    "correlation_matrix = np.corrcoef(expression_data)\n",
    "\n",
    "# Threshold for value-based graph construction\n",
    "delta_values = [0.15, 0.5, 0.85]\n",
    "\n",
    "# Initialize array to store results for each delta value\n",
    "results = []\n",
    "\n",
    "for delta in delta_values:\n",
    "    threshold = delta\n",
    "    adjacency_matrix = (correlation_matrix >= threshold ).astype(int)\n",
    "\n",
    "    # Create a graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes with Level 1 classification as attributes\n",
    "    for idx, gene in enumerate(genes):\n",
    "        level_2_class = gene_to_level_2.get(gene, None)  # Get Level_2 if it exists, otherwise None\n",
    "        G.add_node(gene, level_1=level_1_classes[idx], level_2=level_2_class)\n",
    "\n",
    "    # Add edges based on Pearson correlation threshold\n",
    "    edges = []\n",
    "    for i in range(len(genes)):\n",
    "        for j in range(i + 1, len(genes)):\n",
    "            if adjacency_matrix[i, j] == 1:\n",
    "                pearson_coefficient = correlation_matrix[i, j]\n",
    "                G.add_edge(genes[i], genes[j], weight=pearson_coefficient)\n",
    "\n",
    "    # Remove singletons\n",
    "    singletons = list(nx.isolates(G))\n",
    "    G.remove_nodes_from(singletons)\n",
    "\n",
    "    # Compute additional graph properties\n",
    "    avg_degree = sum(dict(G.degree()).values()) / len(G.nodes())  # Average degree\n",
    "    density = nx.density(G)  # Graph density\n",
    "    avg_clustering = nx.average_clustering(G)  # Average clustering coefficient\n",
    "    largest_cc = max(nx.connected_components(G), key=len)  # Largest connected component\n",
    "    largest_cc_size = len(largest_cc)  # Size of the largest component\n",
    "    diameter = nx.diameter(G.subgraph(largest_cc)) if nx.is_connected(G) else None  # Diameter\n",
    "\n",
    "    # Store results for the current delta, including the graph\n",
    "    results.append({\n",
    "        \"Delta\": delta,\n",
    "        \"Graph\": G,  # Store the graph object\n",
    "        \"Number of Nodes\": len(G.nodes()),\n",
    "        \"Number of Edges\": G.number_of_edges(),\n",
    "        \"Transitivity\": nx.transitivity(G) if len(G.nodes()) > 0 else 0,\n",
    "        \"Singletons Removed\": len(singletons),\n",
    "        \"Average Degree\": avg_degree,\n",
    "        \"Density\": density,\n",
    "        \"Average Clustering Coefficient\": avg_clustering,\n",
    "        \"Largest Component Size\": largest_cc_size,\n",
    "        \"Diameter (if connected)\": diameter\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame for visualization (excluding the graph object)\n",
    "graph_properties = pd.DataFrame([{k: v for k, v in result.items() if k != \"Graph\"} for result in results])\n",
    "\n",
    "output_dir = \"../results/level_1\"\n",
    "if not os.path.exists(output_dir):\n",
    "\tos.makedirs(output_dir)\n",
    "\n",
    "# Save graph properties to a CSV file\n",
    "graph_properties.to_csv(f\"{output_dir}/graph_properties.csv\", index=False)\n",
    "graph_properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 1 Network Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLEAU_COLORS = {\n",
    "    'tab:blue': '#1f77b4',\n",
    "    'tab:pink': '#ff91d7',\n",
    "    'tab:green': '#2ca02c',\n",
    "    'tab:red': '#d62728',\n",
    "    'tab:yellow': '#fff06b',\n",
    "}\n",
    "# Define unique colors for each Level 1 classification\n",
    "unique_classes = level_1_classes.unique()\n",
    "color_map = {cls: TABLEAU_COLORS[list(TABLEAU_COLORS.keys())[i]] for i, cls in enumerate(unique_classes)}\n",
    "\n",
    "# Use the graph with delta = 0.85\n",
    "G = results[2][\"Graph\"]  # Choose the graph with delta = 0.85\n",
    "\n",
    "# Assign colors to nodes based on Level 1 classification\n",
    "node_colors = [color_map[G.nodes[n][\"level_1\"]] for n in G.nodes()]\n",
    "\n",
    "# Edge weights for visualization (stronger edges appear tighter)\n",
    "edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "\n",
    "# Generate graph layout\n",
    "plt.figure(figsize=(10, 10))\n",
    "cell_cycle_phases = [\"Early G1 Phase\", \"Late G1 Phase\", \"S Phase\", \"G2 Phase\", \"M Phase\"]\n",
    "legend_handles = [Patch(color=color, label=f'{cell_cycle_phases[cls-1]}') for cls, color in color_map.items()]\n",
    "plt.legend(handles=legend_handles, loc=\"best\", fontsize=8, title=\"Level 1 Classification\")\n",
    "pos = nx.spring_layout(G, weight='weight', k=0.1, seed=10)  # Spring layout based on weights\n",
    "\n",
    "# Draw nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=200, alpha=1, edgecolors=\"black\")\n",
    "\n",
    "# Draw edges with thickness based on weight\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.25, edge_color=\"black\", width=[w * 2 for w in edge_weights])\n",
    "\n",
    "# Show plot\n",
    "# plt.title(f\"Cho Gene Co-Expression Network (Value-Based, δ = {delta})\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"../results/level_1/visualization/level1_gcn.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 2 Network Visualization\n",
    "For this section, nodes/genes in the dataset that do not have a Level 2 classification will be colored gray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_colors = {cls: TABLEAU_COLORS[list(TABLEAU_COLORS.keys())[i]]\n",
    "               for i, cls in enumerate(unique_classes)}\n",
    "\n",
    "# Function to generate distinct but related colors for Level 2 classifications\n",
    "def generate_hue_variations(base_color, num_shades):\n",
    "    \"\"\"Generate distinct hues within a color family using HSL space.\"\"\"\n",
    "    base_rgb = mcolors.to_rgb(base_color)\n",
    "    base_hls = colorsys.rgb_to_hls(*base_rgb)\n",
    "    \n",
    "    # Create variations by adjusting only the HUE component\n",
    "    hues = np.linspace(base_hls[0] - 0.1, base_hls[0] + 0.1, num_shades)\n",
    "    hues = np.clip(hues, 0, 1)  # Ensure hues are within valid range\n",
    "    \n",
    "    return [mcolors.to_hex(colorsys.hls_to_rgb(hue, base_hls[1], base_hls[2])) for hue in hues]\n",
    "\n",
    "# Generate Level 2 colors based on hue variations within the Level 1 color family\n",
    "level_2_groups = level_2_df.groupby(\"Level_1\")[\"Level_2\"].unique()\n",
    "level_2_colors = {}\n",
    "\n",
    "for lvl1, lvl2_values in level_2_groups.items():\n",
    "    shades = generate_hue_variations(base_colors[lvl1], len(lvl2_values))\n",
    "    for idx, lvl2 in enumerate(lvl2_values):\n",
    "        level_2_colors[lvl2] = shades[idx]\n",
    "\n",
    "# Assign colors to nodes based on Level 2 classification\n",
    "node_colors_level_2 = [\n",
    "    level_2_colors[G.nodes[n][\"level_2\"]] if G.nodes[n][\"level_2\"] in level_2_colors else \"#808080\"  # Gray color\n",
    "    for n in G.nodes()\n",
    "]\n",
    "\n",
    "# Edge weights for visualization\n",
    "edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "\n",
    "# Generate graph layout\n",
    "plt.figure(figsize=(10, 10))\n",
    "cell_cycle_phases = [\"Early G1 Phase\", \"Late G1 Phase\", \"S Phase\", \"G2 Phase\", \"M Phase\"]\n",
    "legend_handles_lvl2 = [Patch(color=color, label=f'Level 2: {cls}') for cls, color in level_2_colors.items()]\n",
    "# plt.legend(handles=legend_handles_lvl2, loc=\"best\", fontsize=6, title=\"Classification Hierarchy\")\n",
    "\n",
    "pos = nx.spring_layout(G, weight='weight', k=0.1, seed=10)\n",
    "\n",
    "# Draw nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_colors_level_2, node_size=200, alpha=1, edgecolors=\"black\")\n",
    "\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.25, edge_color=\"black\", width=[w * 2 for w in edge_weights])\n",
    "\n",
    "# Show plot\n",
    "# plt.title(f\"Cho Gene Co-Expression Network (Value-Based, δ = {delta})\")\n",
    "plt.savefig(\"Network Graphs/cho_gene_coexpression_level2_dataset_level2.png\", dpi=300)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community Detection Auxiliary Functions\n",
    "\n",
    "These are the functions used during the Community Detection of our constructed network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_community(G, communities, title, output_dir, file_name, save_file=False):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    pos = nx.spring_layout(G)  # Recalculate layout to ensure all nodes are included\n",
    "\n",
    "    cmap = plt.get_cmap('viridis', len(communities))\n",
    "\n",
    "    for i, community in enumerate(communities):\n",
    "        if community:  # Check if community is not empty\n",
    "            # Filter out nodes not present in the graph\n",
    "            valid_nodes = [node for node in community if node in G]\n",
    "            nx.draw_networkx_nodes(G, pos, nodelist=valid_nodes, node_color=[cmap(i)], alpha=0.8)\n",
    "\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5, edge_color='gray')\n",
    "    legend_handles = [mpatches.Patch(color=cmap(i), label=f'Community {i + 1}') for i in range(len(communities))]\n",
    "    plt.legend(handles=legend_handles, loc='upper left', title=\"Communities\")\n",
    "    plt.title(f\"{title} Algorithm Communities\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if save_file:\n",
    "        plt.savefig(f\"{output_dir}/{file_name}_{title}.png\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_disconnected_communities(G, communities, color_map, title=\"Paris Algorithm Communities\"):\n",
    "    \"\"\"\n",
    "    Visualizes detected communities as separate disconnected subgraphs while keeping Level 1 colors.\n",
    "\n",
    "    Parameters:\n",
    "    - G: NetworkX graph\n",
    "    - communities: CDLib NodeClustering object (Paris, Infomap, Louvain, etc.)\n",
    "    - color_map: Dictionary mapping Level 1 categories to colors\n",
    "    - title: Title for the plot\n",
    "    \"\"\"\n",
    "\n",
    "    # Define layout positioning for separate communities\n",
    "    num_communities = len(communities.communities)\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Generate separate layouts for each community\n",
    "    layouts = []\n",
    "    for i in range(num_communities):\n",
    "        layouts.append(nx.spring_layout(G.subgraph(communities.communities[i]), seed=42))\n",
    "\n",
    "    # Normalize layout positions to avoid overlap\n",
    "    spacing = 3\n",
    "    for i, layout in enumerate(layouts):\n",
    "        for node in layout:\n",
    "            layout[node] += [i * spacing, 0]  # Shift positions to separate communities\n",
    "\n",
    "    # Draw each community separately\n",
    "    for i, community in enumerate(communities.communities):\n",
    "        valid_nodes = [node for node in community if node in G]\n",
    "        if not valid_nodes:\n",
    "            continue  # Skip empty communities\n",
    "        subG = G.subgraph(valid_nodes)\n",
    "        pos = layouts[i]\n",
    "\n",
    "        # Assign Level 1 colors\n",
    "        node_colors = [color_map[G.nodes[n][\"level_1\"]] for n in subG.nodes()]\n",
    "\n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(subG, pos, node_color=node_colors, node_size=200, alpha=1, edgecolors=\"black\")\n",
    "\n",
    "        # Draw edges\n",
    "        nx.draw_networkx_edges(subG, pos, alpha=0.3, edge_color=\"black\")\n",
    "\n",
    "    # Create a legend for Level 1 classification\n",
    "    cell_cycle_phases = [\"Early G1 Phase\", \"Late G1 Phase\", \"S Phase\", \"G2 Phase\", \"M Phase\"]\n",
    "    legend_handles = [Patch(color=color, label=f'{cell_cycle_phases[cls-1]}') for cls, color in color_map.items()]\n",
    "    plt.legend(handles=legend_handles, loc=\"best\", fontsize=8, title=\"Level 1 Classification\")\n",
    "\n",
    "    # Display\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def modified_rand_score(labels_true, labels_pred, nodes_true_list, nodes_pred_list):\n",
    "    \"\"\"\n",
    "    Compute the Modified Rand Index (MRI), an extension of the Rand Index\n",
    "    that accounts for missing elements between partitions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels_true : list\n",
    "        True labels corresponding to the nodes.\n",
    "    labels_pred : list\n",
    "        Predicted labels corresponding to the nodes.\n",
    "    nodes_true_list : list\n",
    "        An ordered list of nodes corresponding to labels_true.\n",
    "    nodes_pred_list : list\n",
    "        An ordered list of nodes corresponding to labels_pred.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mri : float\n",
    "        The Modified Rand Index score.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(labels_true)\n",
    "    total_pairs = n * (n - 1) // 2  # All possible unique node pairs\n",
    "    n_00, n_11, n_xx = 0, 0, 0  # Initialize counts\n",
    "\n",
    "    # Convert lists to sets for fast lookup\n",
    "    nodes_true_set = set(nodes_true_list)\n",
    "    nodes_pred_set = set(nodes_pred_list)\n",
    "\n",
    "    # Identify common nodes between the two partitions\n",
    "    common_nodes = nodes_true_set.intersection(nodes_pred_set)\n",
    "\n",
    "    # Iterate over all node pairs\n",
    "    for i, j in itertools.combinations(range(n), 2):  # Iterate over all unique pairs\n",
    "        node_i, node_j = nodes_true_list[i], nodes_true_list[j]\n",
    "\n",
    "        # If either node is missing from the common set, count this pair as n_xx.\n",
    "        if node_i not in common_nodes or node_j not in common_nodes:\n",
    "            n_xx += 1\n",
    "        else:\n",
    "            same_true = (labels_true[i] == labels_true[j])\n",
    "            same_pred = (labels_pred[i] == labels_pred[j])\n",
    "            if same_true and same_pred:\n",
    "                n_11 += 1  # Agreeing pair (same in both)\n",
    "            elif not same_true and not same_pred:\n",
    "                n_00 += 1  # Agreeing pair (different in both)\n",
    "\n",
    "    # Compute MRI Score\n",
    "    if total_pairs == 0:\n",
    "        return 1.0  # Perfect match in trivial cases\n",
    "\n",
    "    MRI = (n_00 + n_11 + n_xx) / total_pairs\n",
    "    return MRI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Community Detection\n",
    "\n",
    "Using our formed network, we employ the Paris and LFM algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paris Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paris_communities = algorithms.paris(G)\n",
    "\n",
    "def evaluate_paris():\n",
    "\t# Compute required metrics\n",
    "\tnum_communities = len(paris_communities.communities)\n",
    "\tlargest_community_size = max(len(c) for c in paris_communities.communities)\n",
    "\tcommunity_sizes = [len(c) for c in paris_communities.communities]\n",
    "\tnum_singletons = sum(1 for c in paris_communities.communities if len(c) == 1)\n",
    "\tmodularity_score = evaluation.newman_girvan_modularity(G, paris_communities).score\n",
    "\tcommunity_centralities = []\n",
    "\tfor community in paris_communities.communities:\n",
    "\t\t\tif len(community) > 1:  # avoid singleton closeness warnings\n",
    "\t\t\t\t\tsubgraph = G.subgraph(community)\n",
    "\t\t\t\t\tcentralities = nx.closeness_centrality(subgraph).values()\n",
    "\t\t\t\t\tcommunity_centralities.extend(centralities)\n",
    "\tavg_closeness_centrality = np.mean(community_centralities) if community_centralities else 0.0\n",
    "\n",
    "\tpredicted_labels_dict = {node: -1 for node in G.nodes()}\n",
    "\tfor i, community in enumerate(paris_communities.communities):\n",
    "\t\t\tfor node in community:\n",
    "\t\t\t\t\tpredicted_labels_dict[node] = i\n",
    "\tpredicted_labels = [predicted_labels_dict[node] for node in G.nodes()]\n",
    "\n",
    "\tnode_to_community = {node: idx for idx, community in enumerate(paris_communities.communities) for node in community}\n",
    "\ttrue_labels_level_1 = [G.nodes[n][\"level_1\"] for n in G.nodes()]\n",
    "\t# true_labels_level_2 = [G.nodes[n][\"level_2\"] for n in G.nodes()]\n",
    "\n",
    "\t# ARI calculation\n",
    "\tari_classifications_score1 = adjusted_rand_score(true_labels_level_1, predicted_labels)\n",
    "\t# ari_classifications_score2 = adjusted_rand_score(true_labels_level_2, predicted_labels)\n",
    "\n",
    "\t# RI calculation\n",
    "\tri_classifications_score1 = rand_score(true_labels_level_1, predicted_labels)\n",
    "\t# ri_classifications_score2 = rand_score(true_labels_level_2, predicted_labels)\n",
    "\n",
    "\t# MRI calculation\n",
    "\tnodes_true_list = list(G.nodes())\n",
    "\tnodes_pred_list = list(node for community in paris_communities.communities for node in community)\n",
    "\tmri_classifications_score1 = modified_rand_score(true_labels_level_1, predicted_labels, nodes_true_list, nodes_pred_list)\n",
    "\t# mri_classifications_score2 = modified_rand_score(true_labels_level_2, predicted_labels, nodes_true_list, nodes_pred_list)\n",
    "\n",
    "\tParis_results = pd.DataFrame({\n",
    "\t\t\t\"Number of Communities\": [num_communities],\n",
    "\t\t\t\"Nodes in Largest Community\": [largest_community_size],\n",
    "\t\t\t\"Community Sizes\": [community_sizes],\n",
    "\t\t\t\"Singletons\": [num_singletons],\n",
    "\t\t\t\"Modularity\": [modularity_score],\n",
    "\t\t\t\"Closeness Centrality\": [avg_closeness_centrality],\n",
    "\t\t\t\"ARI (Level 1)\": [ari_classifications_score1],\n",
    "\t\t\t# \"ARI (Level 2)\": [ari_classifications_score2],\n",
    "\t\t\t\"RI (Level 1)\": [ri_classifications_score1],\n",
    "\t\t\t# \"RI (Level 2)\": [ri_classifications_score2],\n",
    "\t\t\t\"MRI (Level 1)\": [mri_classifications_score1],\n",
    "\t\t\t# \"MRI (Level 2)\": [mri_classifications_score2],\n",
    "\t\t\t\"Communities\": [paris_communities.communities]\n",
    "\t})\n",
    "\treturn Paris_results\n",
    "Paris_results = evaluate_paris()\n",
    "# output_dir = \"Paris - Graphs\"\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "# Paris_results.to_csv(f\"{output_dir}/Paris_Results_Cho.csv\")\n",
    "Paris_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_disconnected_communities(G, paris_communities, color_map, title=\"Level 1 Network: Paris Algorithm Communities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "alpha_values = np.arange(0.1, 1.0, 0.1)\n",
    "lfm_results_list = []\n",
    "lfm_communities = []\n",
    "for alpha in alpha_values:\n",
    "\tlfm_alpha = alpha\n",
    "\tcurr_community = algorithms.lfm(G, alpha=lfm_alpha)\n",
    "\tlfm_communities.append(curr_community)\n",
    "\n",
    "\tdef evaluate_lfm():\n",
    "\t\t# Compute required metrics\n",
    "\t\tnum_communities = len(curr_community.communities)\n",
    "\t\tlargest_community_size = max(len(c) for c in curr_community.communities)\n",
    "\t\tcommunity_sizes = [len(c) for c in curr_community.communities]\n",
    "\t\tnum_singletons = sum(1 for c in curr_community.communities if len(c) == 1)\n",
    "\t\tmodularity_score = evaluation.newman_girvan_modularity(G, curr_community).score\n",
    "\t\tcommunity_centralities = []\n",
    "\t\tfor community in curr_community.communities:\n",
    "\t\t\t\tif len(community) > 1:  # closeness is undefined for single-node graphs\n",
    "\t\t\t\t\t\tsubgraph = G.subgraph(community)\n",
    "\t\t\t\t\t\tcentralities = nx.closeness_centrality(subgraph).values()\n",
    "\t\t\t\t\t\tcommunity_centralities.extend(centralities)\n",
    "\t\tavg_closeness_centrality = np.mean(community_centralities)\n",
    "\n",
    "\n",
    "\t\tnode_counts = {}\n",
    "\t\tfor i, community in enumerate(curr_community.communities):\n",
    "\t\t\tfor node in community:\n",
    "\t\t\t\tif node not in node_counts:\n",
    "\t\t\t\t\tnode_counts[node] = []\n",
    "\t\t\t\tnode_counts[node].append(i)\n",
    "\n",
    "\t\t# Assign each node to the **smallest indexed** community it appears in\n",
    "\t\tpredicted_labels_dict = {node: min(communities) for node, communities in node_counts.items()}\n",
    "\n",
    "\t\t# Create the final predicted labels list\n",
    "\t\tpredicted_labels = [predicted_labels_dict[n] for n in G.nodes()]\n",
    "\n",
    "\t\tnode_to_community = {node: idx for idx, community in enumerate(curr_community.communities) for node in community}\n",
    "\t\ttrue_labels_level_1 = [G.nodes[n][\"level_1\"] for n in G.nodes()]\n",
    "\n",
    "\t\t# ARI calculation\n",
    "\t\tari_classifications_score1 = adjusted_rand_score(true_labels_level_1, predicted_labels)\n",
    "\n",
    "\t\t# RI calculation\n",
    "\t\tri_classifications_score1 = rand_score(true_labels_level_1, predicted_labels)\n",
    "\n",
    "\t\t# MRI calculation\n",
    "\t\tnodes_true_list = list(G.nodes())\n",
    "\t\tnodes_pred_list = list(node for community in curr_community.communities for node in community)\n",
    "\t\tmri_classifications_score1 = modified_rand_score(true_labels_level_1, predicted_labels, nodes_true_list, nodes_pred_list)\n",
    "\n",
    "\t\tLFM_results = pd.DataFrame({\n",
    "\t\t\t\"Alpha\": [lfm_alpha],\n",
    "\t\t\t\"Number of Communities\": [num_communities],\n",
    "\t\t\t\"Nodes in Largest Community\": [largest_community_size],\n",
    "\t\t\t\"Community Sizes\": [community_sizes],\n",
    "\t\t\t\"Singletons\": [num_singletons],\n",
    "\t\t\t\"Modularity\": [modularity_score],\n",
    "\t\t\t\"Closeness Centrality\": [avg_closeness_centrality],\n",
    "\t\t\t\"ARI (Level 1)\": [ari_classifications_score1],\n",
    "\t\t\t\"RI (Level 1)\": [ri_classifications_score1],\n",
    "\t\t\t\"MRI (Level 1)\": [mri_classifications_score1],\n",
    "\t\t\t\"Communities\": [curr_community.communities]\n",
    "\t\t})\n",
    "\t\treturn LFM_results\n",
    "\n",
    "\tlfm_results_list.append(evaluate_lfm())\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "LFM_results = pd.concat(lfm_results_list, ignore_index=True)\n",
    "LFM_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the best performing LFM alpha value (alpha = 0.7)\n",
    "best_lfm_communities = lfm_communities[6]\n",
    "visualize_disconnected_communities(G, best_lfm_communities, color_map, title=f\"Level 1 Network: LFM Algorithm Communities (α = {0.7})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Girvan Newman Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn_levels = range(1, 11)\n",
    "gn_results_list = []\n",
    "gn_communities_list = []\n",
    "\n",
    "for level in gn_levels:\n",
    "\tgn_level = level\n",
    "\tcurr_community = algorithms.girvan_newman(G, level=gn_level)\n",
    "\tgn_communities_list.append(curr_community)\n",
    "\n",
    "\tdef evaluate_gn():\n",
    "\t\t# Compute required metrics\n",
    "\t\tnum_communities = len(curr_community.communities)\n",
    "\t\tlargest_community_size = max(len(c) for c in curr_community.communities)\n",
    "\t\tcommunity_sizes = [len(c) for c in curr_community.communities]\n",
    "\t\tnum_singletons = sum(1 for c in curr_community.communities if len(c) == 1)\n",
    "\t\tmodularity_score = evaluation.newman_girvan_modularity(G, curr_community).score\n",
    "\n",
    "\t\t# Per-community closeness\n",
    "\t\tcommunity_centralities = []\n",
    "\t\tfor community in curr_community.communities:\n",
    "\t\t\tif len(community) > 1:\n",
    "\t\t\t\tsubgraph = G.subgraph(community)\n",
    "\t\t\t\tcentralities = nx.closeness_centrality(subgraph).values()\n",
    "\t\t\t\tcommunity_centralities.extend(centralities)\n",
    "\t\tavg_closeness_centrality = np.mean(community_centralities)\n",
    "\n",
    "\t\t# Assign each node to one community\n",
    "\t\tpredicted_labels_dict = {node: -1 for node in G.nodes()}\n",
    "\t\tfor i, community in enumerate(curr_community.communities):\n",
    "\t\t\tfor node in community:\n",
    "\t\t\t\tpredicted_labels_dict[node] = i\n",
    "\t\tpredicted_labels = [predicted_labels_dict[n] for n in G.nodes()]\n",
    "\t\ttrue_labels_level_1 = [G.nodes[n][\"level_1\"] for n in G.nodes()]\n",
    "\n",
    "\t\t# Metrics\n",
    "\t\tari_classifications_score1 = adjusted_rand_score(true_labels_level_1, predicted_labels)\n",
    "\t\tri_classifications_score1 = rand_score(true_labels_level_1, predicted_labels)\n",
    "\t\tnodes_true_list = list(G.nodes())\n",
    "\t\tnodes_pred_list = list(node for community in curr_community.communities for node in community)\n",
    "\t\tmri_classifications_score1 = modified_rand_score(true_labels_level_1, predicted_labels, nodes_true_list, nodes_pred_list)\n",
    "\n",
    "\t\tGN_results = pd.DataFrame({\n",
    "\t\t\t\"Level\": [gn_level],\n",
    "\t\t\t\"Number of Communities\": [num_communities],\n",
    "\t\t\t\"Nodes in Largest Community\": [largest_community_size],\n",
    "\t\t\t\"Community Sizes\": [community_sizes],\n",
    "\t\t\t\"Singletons\": [num_singletons],\n",
    "\t\t\t\"Modularity\": [modularity_score],\n",
    "\t\t\t\"Closeness Centrality\": [avg_closeness_centrality],\n",
    "\t\t\t\"ARI (Level 1)\": [ari_classifications_score1],\n",
    "\t\t\t\"RI (Level 1)\": [ri_classifications_score1],\n",
    "\t\t\t\"MRI (Level 1)\": [mri_classifications_score1],\n",
    "\t\t\t\"Communities\": [curr_community.communities]\n",
    "\t\t})\n",
    "\t\treturn GN_results\n",
    "\n",
    "\tgn_results_list.append(evaluate_gn())\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "GN_results = pd.concat(gn_results_list, ignore_index=True)\n",
    "GN_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gn_community = gn_communities_list[3]  # Choose the graph with level = 4\n",
    "visualize_disconnected_communities(G, best_gn_community, color_map, title=f\"Level 1 Network: Girvan-Newman Algorithm Communities (GN level = {gn_level})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Community Detection\n",
    "Using our formed network, we employ the Infomap and TILES algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infomap Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infomap_communities = algorithms.infomap(G)\n",
    "\n",
    "def evaluate_infomap():\n",
    "\t# Compute required metrics\n",
    "\tnum_communities = len(infomap_communities.communities)\n",
    "\tlargest_community_size = max(len(c) for c in infomap_communities.communities)\n",
    "\tcommunity_sizes = [len(c) for c in infomap_communities.communities]\n",
    "\tnum_singletons = sum(1 for c in infomap_communities.communities if len(c) == 1)\n",
    "\tmodularity_score = evaluation.newman_girvan_modularity(G, infomap_communities).score\n",
    "\tcommunity_centralities = []\n",
    "\tfor community in curr_community.communities:\n",
    "\t\tif len(community) > 1:\n",
    "\t\t\tsubgraph = G.subgraph(community)\n",
    "\t\t\tcentralities = nx.closeness_centrality(subgraph).values()\n",
    "\t\t\tcommunity_centralities.extend(centralities)\n",
    "\tavg_closeness_centrality = np.mean(community_centralities)\n",
    "\n",
    "\tpredicted_labels_dict = {node: -1 for node in G.nodes()}\n",
    "\tfor i, community in enumerate(infomap_communities.communities):\n",
    "\t\t\tfor node in community:\n",
    "\t\t\t\t\tpredicted_labels_dict[node] = i\n",
    "\tpredicted_labels = [predicted_labels_dict[node] for node in G.nodes()]\n",
    "\n",
    "\n",
    "\tnode_to_community = {node: idx for idx, community in enumerate(infomap_communities.communities) for node in community}\n",
    "\ttrue_labels_level_1 = [G.nodes[n][\"level_1\"] for n in G.nodes()]\n",
    "\n",
    "\t# ARI calculation\n",
    "\tari_classifications_score1 = adjusted_rand_score(true_labels_level_1, predicted_labels)\n",
    "\n",
    "\t# RI calculation\n",
    "\tri_classifications_score1 = rand_score(true_labels_level_1, predicted_labels)\n",
    "\n",
    "\t# MRI calculation\n",
    "\tnodes_true_list = list(G.nodes())\n",
    "\tnodes_pred_list = list(node for community in infomap_communities.communities for node in community)\n",
    "\tmri_classifications_score1 = modified_rand_score(true_labels_level_1, predicted_labels, nodes_true_list, nodes_pred_list)\n",
    "\n",
    "\tInfomap_results = pd.DataFrame({\n",
    "\t\t\t\"Number of Communities\": [num_communities],\n",
    "\t\t\t\"Nodes in Largest Community\": [largest_community_size],\n",
    "\t\t\t\"Community Sizes\": [community_sizes],\n",
    "\t\t\t\"Singletons\": [num_singletons],\n",
    "\t\t\t\"Modularity\": [modularity_score],\n",
    "\t\t\t\"Closeness Centrality\": [avg_closeness_centrality],\n",
    "\t\t\t\"ARI (Level 1)\": [ari_classifications_score1],\n",
    "\t\t\t\"RI (Level 1)\": [ri_classifications_score1],\n",
    "\t\t\t\"MRI (Level 1)\": [mri_classifications_score1],\n",
    "\t\t\t\"Communities\": [infomap_communities.communities]\n",
    "\t})\n",
    "\treturn Infomap_results\n",
    "\n",
    "Infomap_results = evaluate_infomap()\n",
    "# output_dir = \"Infomap - Graphs\"\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "# Infomap_results.to_csv(f\"{output_dir}/Infomap_results_Cho.csv\")\n",
    "Infomap_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_disconnected_communities(G, infomap_communities, color_map, title=\"Level 1 Network: Infomap Algorithm Communities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walktrap Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walktrap_communities = algorithms.walktrap(G)\n",
    "\n",
    "def evaluate_walktrap():\n",
    "\t# Compute required metrics\n",
    "\tnum_communities = len(walktrap_communities.communities)\n",
    "\tlargest_community_size = max(len(c) for c in walktrap_communities.communities)\n",
    "\tcommunity_sizes = [len(c) for c in walktrap_communities.communities]\n",
    "\tnum_singletons = sum(1 for c in walktrap_communities.communities if len(c) == 1)\n",
    "\tmodularity_score = evaluation.newman_girvan_modularity(G, walktrap_communities).score\n",
    "\tcommunity_centralities = []\n",
    "\tfor community in curr_community.communities:\n",
    "\t\tif len(community) > 1:\n",
    "\t\t\tsubgraph = G.subgraph(community)\n",
    "\t\t\tcentralities = nx.closeness_centrality(subgraph).values()\n",
    "\t\t\tcommunity_centralities.extend(centralities)\n",
    "\tavg_closeness_centrality = np.mean(community_centralities)\n",
    "\n",
    "\tpredicted_labels_dict = {node: -1 for node in G.nodes()}\n",
    "\tfor i, community in enumerate(walktrap_communities.communities):\n",
    "\t\t\tfor node in community:\n",
    "\t\t\t\t\tpredicted_labels_dict[node] = i\n",
    "\tpredicted_labels = [predicted_labels_dict[node] for node in G.nodes()]\n",
    "\n",
    "\n",
    "\tnode_to_community = {node: idx for idx, community in enumerate(walktrap_communities.communities) for node in community}\n",
    "\ttrue_labels_level_1 = [G.nodes[n][\"level_1\"] for n in G.nodes()]\n",
    "\t# true_labels_level_2 = [G.nodes[n][\"level_2\"] for n in G.nodes()]\n",
    "\n",
    "\t# ARI calculation\n",
    "\tari_classifications_score1 = adjusted_rand_score(true_labels_level_1, predicted_labels)\n",
    "\t# ari_classifications_score2 = adjusted_rand_score(true_labels_level_2, predicted_labels)\n",
    "\n",
    "\t# RI calculation\n",
    "\tri_classifications_score1 = rand_score(true_labels_level_1, predicted_labels)\n",
    "\t# ri_classifications_score2 = rand_score(true_labels_level_2, predicted_labels)\n",
    "\n",
    "\t# MRI calculation\n",
    "\tnodes_true_list = list(G.nodes())\n",
    "\tnodes_pred_list = list(node for community in walktrap_communities.communities for node in community)\n",
    "\tmri_classifications_score1 = modified_rand_score(true_labels_level_1, predicted_labels, nodes_true_list, nodes_pred_list)\n",
    "\t# mri_classifications_score2 = modified_rand_score(true_labels_level_2, predicted_labels, nodes_true_list, nodes_pred_list)\n",
    "\n",
    "\tWalktrap_results = pd.DataFrame({\n",
    "\t\t\t\"Number of Communities\": [num_communities],\n",
    "\t\t\t\"Nodes in Largest Community\": [largest_community_size],\n",
    "\t\t\t\"Community Sizes\": [community_sizes],\n",
    "\t\t\t\"Singletons\": [num_singletons],\n",
    "\t\t\t\"Modularity\": [modularity_score],\n",
    "\t\t\t\"Closeness Centrality\": [avg_closeness_centrality],\n",
    "\t\t\t\"ARI (Level 1)\": [ari_classifications_score1],\n",
    "\t\t\t# \"ARI (Level 2)\": [ari_classifications_score2],\n",
    "\t\t\t\"RI (Level 1)\": [ri_classifications_score1],\n",
    "\t\t\t# \"RI (Level 2)\": [ri_classifications_score2],\n",
    "\t\t\t\"MRI (Level 1)\": [mri_classifications_score1],\n",
    "\t\t\t# \"MRI (Level 2)\": [mri_classifications_score2],\n",
    "\t\t\t\"Communities\": [walktrap_communities.communities]\n",
    "\t})\n",
    "\treturn Walktrap_results\n",
    "\n",
    "Walktrap_results = evaluate_walktrap()\n",
    "# output_dir = \"Walktrap - Graphs\"\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "# Walktrap_results.to_csv(f\"{output_dir}/Walktrap_results_Cho.csv\")\n",
    "Walktrap_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_disconnected_communities(G, walktrap_communities, color_map, title=\"Level 1 Network: Walktrap Algorithm Communities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spinglass Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spinglass_communities = algorithms.spinglass(G)\n",
    "\n",
    "def evaluate_spinglass():\n",
    "\t# Compute required metrics\n",
    "\tnum_communities = len(spinglass_communities.communities)\n",
    "\tlargest_community_size = max(len(c) for c in spinglass_communities.communities)\n",
    "\tcommunity_sizes = [len(c) for c in spinglass_communities.communities]\n",
    "\tnum_singletons = sum(1 for c in spinglass_communities.communities if len(c) == 1)\n",
    "\tmodularity_score = evaluation.newman_girvan_modularity(G, spinglass_communities).score\n",
    "\tcommunity_centralities = []\n",
    "\tfor community in curr_community.communities:\n",
    "\t\tif len(community) > 1:\n",
    "\t\t\tsubgraph = G.subgraph(community)\n",
    "\t\t\tcentralities = nx.closeness_centrality(subgraph).values()\n",
    "\t\t\tcommunity_centralities.extend(centralities)\n",
    "\tavg_closeness_centrality = np.mean(community_centralities)\n",
    "\n",
    "\tpredicted_labels_dict = {node: -1 for node in G.nodes()}\n",
    "\tfor i, community in enumerate(spinglass_communities.communities):\n",
    "\t\t\tfor node in community:\n",
    "\t\t\t\t\tpredicted_labels_dict[node] = i\n",
    "\tpredicted_labels = [predicted_labels_dict[node] for node in G.nodes()]\n",
    "\n",
    "\n",
    "\tnode_to_community = {node: idx for idx, community in enumerate(spinglass_communities.communities) for node in community}\n",
    "\ttrue_labels_level_1 = [G.nodes[n][\"level_1\"] for n in G.nodes()]\n",
    "\t# true_labels_level_2 = [G.nodes[n][\"level_2\"] for n in G.nodes()]\n",
    "\n",
    "\t# ARI calculation\n",
    "\tari_classifications_score1 = adjusted_rand_score(true_labels_level_1, predicted_labels)\n",
    "\t# ari_classifications_score2 = adjusted_rand_score(true_labels_level_2, predicted_labels)\n",
    "\n",
    "\t# RI calculation\n",
    "\tri_classifications_score1 = rand_score(true_labels_level_1, predicted_labels)\n",
    "\t# ri_classifications_score2 = rand_score(true_labels_level_2, predicted_labels)\n",
    "\n",
    "\t# MRI calculation\n",
    "\tnodes_true_list = list(G.nodes())\n",
    "\tnodes_pred_list = list(node for community in spinglass_communities.communities for node in community)\n",
    "\tmri_classifications_score1 = modified_rand_score(true_labels_level_1, predicted_labels, nodes_true_list, nodes_pred_list)\n",
    "\t# mri_classifications_score2 = modified_rand_score(true_labels_level_2, predicted_labels, nodes_true_list, nodes_pred_list)\n",
    "\n",
    "\tSpinglass_results = pd.DataFrame({\n",
    "\t\t\t\"Number of Communities\": [num_communities],\n",
    "\t\t\t\"Nodes in Largest Community\": [largest_community_size],\n",
    "\t\t\t\"Community Sizes\": [community_sizes],\n",
    "\t\t\t\"Singletons\": [num_singletons],\n",
    "\t\t\t\"Modularity\": [modularity_score],\n",
    "\t\t\t\"Closeness Centrality\": [avg_closeness_centrality],\n",
    "\t\t\t\"ARI (Level 1)\": [ari_classifications_score1],\n",
    "\t\t\t# \"ARI (Level 2)\": [ari_classifications_score2],\n",
    "\t\t\t\"RI (Level 1)\": [ri_classifications_score1],\n",
    "\t\t\t# \"RI (Level 2)\": [ri_classifications_score2],\n",
    "\t\t\t\"MRI (Level 1)\": [mri_classifications_score1],\n",
    "\t\t\t# \"MRI (Level 2)\": [mri_classifications_score2],\n",
    "\t\t\t\"Communities\": [spinglass_communities.communities]\n",
    "\t})\n",
    "\treturn Spinglass_results\n",
    "\n",
    "Spinglass_results = evaluate_spinglass()\n",
    "# output_dir = \"Spinglass - Graphs\"\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "# Spinglass_results.to_csv(f\"{output_dir}/Spinglass_results_Cho.csv\")\n",
    "Spinglass_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_disconnected_communities(G, spinglass_communities, color_map, title=\"Cho Network: Spinglass Algorithm Communities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TILES Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TILES!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community Detection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Community_Detection_Results = pd.concat([Paris_results, LFM_results.iloc[[6]], GN_results.iloc[[3]], Infomap_results, Walktrap_results], axis=0)\n",
    "Community_Detection_Results.index = [\"Paris\", \"LFM\", \"Girvan-Newman\", \"Infomap\", \"Walktrap\"]\n",
    "\n",
    "output_dir = \"../results/level_1/community_detection_results\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "Community_Detection_Results.to_csv(f\"{output_dir}/level1_cd_results.csv\")\n",
    "\n",
    "Community_Detection_Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
